{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **KV Cache in Transformer Decoders**\n",
    "### **1. What is KV Cache?**\n",
    "The **KV Cache** (Key-Value Cache) is an optimization technique used in **autoregressive decoding** to **reuse previously computed key (K) and value (V) matrices** instead of recomputing them at every decoding step. This significantly reduces **computational cost** and **memory usage** in large transformer models like **GPT, LLaMA, and ChatGPT**.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Why Do We Need KV Cache?**\n",
    "### **The Problem in Standard Decoding**\n",
    "In an **autoregressive transformer decoder**, each token in the sequence is generated **one-by-one**, and at every step \\( t \\), we compute:\n",
    "\n",
    "1. **Query (Q), Key (K), and Value (V)** matrices:\n",
    "   \\[\n",
    "   Q_t = X_t W_Q, \\quad K_t = X_t W_K, \\quad V_t = X_t W_V\n",
    "   \\]\n",
    "2. Compute **self-attention**:\n",
    "   \\[\n",
    "   A_t = \\text{softmax} \\left( \\frac{Q_t K^T}{\\sqrt{d_k}} \\right) V\n",
    "   \\]\n",
    "\n",
    "At each step \\( t \\), the decoder needs to **recompute** all previous keys and values to perform attention, leading to **quadratic complexity** \\( O(N^2 d) \\).\n",
    "\n",
    "---\n",
    "### **How KV Cache Fixes This**\n",
    "Instead of recomputing **all** keys and values from scratch at every step, we **store the past keys and values** and only append new ones.\n",
    "\n",
    "1. **Store Previous \\( K, V \\) in Memory**:\n",
    "   \\[\n",
    "   K_{\\text{cache}} = [K_1, K_2, ..., K_{t-1}]\n",
    "   \\]\n",
    "   \\[\n",
    "   V_{\\text{cache}} = [V_1, V_2, ..., V_{t-1}]\n",
    "   \\]\n",
    "2. **At Step \\( t \\), Only Compute for New Token**:\n",
    "   - Compute **only \\( K_t \\) and \\( V_t \\)**.\n",
    "   - Append them to the cache:\n",
    "     \\[\n",
    "     K_{\\text{cache}} = [K_{\\text{cache}}, K_t]\n",
    "     \\]\n",
    "     \\[\n",
    "     V_{\\text{cache}} = [V_{\\text{cache}}, V_t]\n",
    "     \\]\n",
    "3. **Compute Attention Efficiently**:\n",
    "   \\[\n",
    "   A_t = \\text{softmax} \\left( \\frac{Q_t K_{\\text{cache}}^T}{\\sqrt{d_k}} \\right) V_{\\text{cache}}\n",
    "   \\]\n",
    "\n",
    "Now, instead of recomputing all previous key-value pairs, the model **only updates the new token's values and performs attention efficiently**.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Mathematical Formulation of KV Cache**\n",
    "### **Without KV Cache (Recomputing Each Step)**\n",
    "At every step \\( t \\), standard self-attention requires:\n",
    "\\[\n",
    "A_t = \\text{softmax} \\left( \\frac{Q_t K^T}{\\sqrt{d_k}} \\right) V\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( Q_t \\) is the query for the current token.\n",
    "- \\( K = [K_1, ..., K_t] \\) (computed fresh at every step).\n",
    "- \\( V = [V_1, ..., V_t] \\).\n",
    "\n",
    "This leads to \\( O(N^2 d) \\) complexity.\n",
    "\n",
    "---\n",
    "\n",
    "### **With KV Cache (Efficient Computation)**\n",
    "Instead of recalculating old \\( K \\) and \\( V \\), we store:\n",
    "\n",
    "\\[\n",
    "K_{\\text{cache}} = [K_1, K_2, ..., K_{t-1}]\n",
    "\\]\n",
    "\\[\n",
    "V_{\\text{cache}} = [V_1, V_2, ..., V_{t-1}]\n",
    "\\]\n",
    "\n",
    "Then, at **step \\( t \\)**:\n",
    "1. Compute only **new key and value**:\n",
    "   \\[\n",
    "   K_t = X_t W_K, \\quad V_t = X_t W_V\n",
    "   \\]\n",
    "2. Append to the cache:\n",
    "   \\[\n",
    "   K_{\\text{cache}} = [K_{\\text{cache}}, K_t], \\quad V_{\\text{cache}} = [V_{\\text{cache}}, V_t]\n",
    "   \\]\n",
    "3. Compute **attention only on stored values**:\n",
    "   \\[\n",
    "   A_t = \\text{softmax} \\left( \\frac{Q_t K_{\\text{cache}}^T}{\\sqrt{d_k}} \\right) V_{\\text{cache}}\n",
    "   \\]\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Memory Usage of KV Cache**\n",
    "Since we store all previous keys and values, memory usage **grows linearly** with sequence length \\( N \\).\n",
    "\n",
    "### **Memory Calculation**\n",
    "Each token stores:\n",
    "\\[\n",
    "K_t \\in \\mathbb{R}^{d_k}, \\quad V_t \\in \\mathbb{R}^{d_v}\n",
    "\\]\n",
    "\n",
    "For a **batch size \\( B \\)**, **number of heads \\( H \\)**, and **sequence length \\( N \\)**:\n",
    "- \\( K_{\\text{cache}} \\) has shape \\( (B, H, N, d_k) \\).\n",
    "- \\( V_{\\text{cache}} \\) has shape \\( (B, H, N, d_v) \\).\n",
    "\n",
    "Thus, the **total KV cache memory** is:\n",
    "\\[\n",
    "\\text{Memory} = B \\times H \\times N \\times (d_k + d_v) \\times \\text{data type size}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **Example: GPT-4 Memory Usage**\n",
    "Assume:\n",
    "- Batch size **\\( B = 4 \\)**\n",
    "- Heads **\\( H = 16 \\)**\n",
    "- Sequence length **\\( N = 2048 \\)**\n",
    "- Key/Value size **\\( d_k = d_v = 64 \\)**\n",
    "- Data type **FP16 (2 bytes per element)**\n",
    "\n",
    "#### **Compute Memory for KV Cache**\n",
    "\\[\n",
    "\\text{Memory} = 4 \\times 16 \\times 2048 \\times (64 + 64) \\times 2\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "= 4 \\times 16 \\times 2048 \\times 128 \\times 2\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "= 4 \\times 16 \\times 524,288\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "= 33,554,432 \\text{ bytes} = 32MB\n",
    "\\]\n",
    "\n",
    "For **longer sequences (e.g., 8K tokens)**, memory grows **linearly**.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Advantages of KV Cache**\n",
    "‚úÖ **Speeds Up Decoding** ‚Üí Avoids recomputation, reducing latency.  \n",
    "‚úÖ **Reduces Computational Cost** ‚Üí No need to multiply against all past tokens.  \n",
    "‚úÖ **Efficient for Large Models** ‚Üí Used in **GPT-3, GPT-4, LLaMA, ChatGPT**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **6. KV Cache vs. No KV Cache**\n",
    "| Feature | Without KV Cache | With KV Cache |\n",
    "|---------|-----------------|--------------|\n",
    "| **Computation** | \\( O(N^2 d) \\) | \\( O(N d) \\) |\n",
    "| **Memory Usage** | Lower | Higher (stores all past K, V) |\n",
    "| **Decoding Speed** | Slow | Fast |\n",
    "| **Used In** | RNNs, Small Transformers | GPT-3, LLaMA, ChatGPT |\n",
    "\n",
    "üöÄ **KV Caching makes large-scale transformers efficient for text generation!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **KV Cache Optimization: Shared KV Cache (ÂÖ±Áî® KV Cache)**\n",
    "Using a **shared KV cache (ÂÖ±Áî® KV Cache)** is an optimization strategy that allows multiple queries (Q) to use the same precomputed **key (K) and value (V) matrices**, reducing redundant computations and improving efficiency in transformer decoders.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. What is Shared KV Cache?**\n",
    "### **Standard KV Cache vs. Shared KV Cache**\n",
    "- **Standard KV Cache**: Each sequence in a batch has its own independent **K and V** cache.\n",
    "- **Shared KV Cache**: Multiple sequences **reuse the same** precomputed **K, V** cache to **reduce memory usage**.\n",
    "\n",
    "### **Why Use Shared KV Cache?**\n",
    "1. **Lower Memory Footprint**  \n",
    "   - Instead of storing separate KV caches for each sequence, we **share one cache** across multiple decoder steps or sequences.\n",
    "  \n",
    "2. **Faster Decoding**  \n",
    "   - Since the **same K and V** are used across different queries, we avoid recomputing keys/values for every new sequence.\n",
    "\n",
    "3. **Ideal for Multi-Turn Chatbots & Beam Search**  \n",
    "   - When generating multiple responses for the same prompt, shared KV cache allows efficient reuse of **previous K, V states**.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Mathematical Explanation of Shared KV Cache**\n",
    "### **Standard KV Cache (Without Sharing)**\n",
    "For a sequence of length \\( N \\), at each decoding step \\( t \\):\n",
    "\n",
    "1. Compute **new K and V**:\n",
    "   \\[\n",
    "   K_t = X_t W_K, \\quad V_t = X_t W_V\n",
    "   \\]\n",
    "\n",
    "2. Append to KV Cache:\n",
    "   \\[\n",
    "   K_{\\text{cache}} = [K_{\\text{cache}}, K_t], \\quad V_{\\text{cache}} = [V_{\\text{cache}}, V_t]\n",
    "   \\]\n",
    "\n",
    "3. Compute Attention:\n",
    "   \\[\n",
    "   A_t = \\text{softmax} \\left( \\frac{Q_t K_{\\text{cache}}^T}{\\sqrt{d_k}} \\right) V_{\\text{cache}}\n",
    "   \\]\n",
    "\n",
    "This is done **independently** for each sequence, leading to redundant storage of similar \\( K \\) and \\( V \\).\n",
    "\n",
    "---\n",
    "\n",
    "### **Shared KV Cache (Optimized)**\n",
    "Instead of storing separate KV caches, we **reuse one shared cache** across multiple sequences or decoding steps.\n",
    "\n",
    "1. Store **only one copy** of \\( K, V \\) for multiple sequences:\n",
    "   \\[\n",
    "   K_{\\text{shared}} = K_{\\text{cache}}, \\quad V_{\\text{shared}} = V_{\\text{cache}}\n",
    "   \\]\n",
    "\n",
    "2. Compute attention **using the shared cache**:\n",
    "   \\[\n",
    "   A_t = \\text{softmax} \\left( \\frac{Q_t K_{\\text{shared}}^T}{\\sqrt{d_k}} \\right) V_{\\text{shared}}\n",
    "   \\]\n",
    "\n",
    "3. No need to **duplicate K and V** for each sequence.\n",
    "\n",
    "This reduces **memory overhead from \\( O(BHN) \\) to \\( O(HN) \\)**, where:\n",
    "- \\( B \\) = batch size\n",
    "- \\( H \\) = number of attention heads\n",
    "- \\( N \\) = sequence length\n",
    "\n",
    "---\n",
    "\n",
    "## **3. When to Use Shared KV Cache?**\n",
    "### **Use Case 1: Beam Search**\n",
    "- In **beam search**, we generate multiple sequences **from the same prompt**.\n",
    "- Instead of storing **separate KV caches for each beam**, we **share one KV cache** across all beams.\n",
    "  \n",
    "### **Use Case 2: Multi-Turn Conversations**\n",
    "- When responding to **multiple follow-up queries** in a chatbot, we can **reuse** the previous KV cache instead of recomputing it.\n",
    "  \n",
    "### **Use Case 3: Parallel Decoding in Multi-Agent Models**\n",
    "- In **multi-agent transformer systems**, different agents may **reference the same past context**, making shared KV cache **efficient**.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Memory Comparison: Standard vs. Shared KV Cache**\n",
    "### **Standard KV Cache Memory Usage**\n",
    "Each sequence stores **independent** KV caches:\n",
    "\n",
    "\\[\n",
    "\\text{Memory} = B \\times H \\times N \\times (d_k + d_v) \\times 2\n",
    "\\]\n",
    "\n",
    "For **batch size \\( B = 4 \\), heads \\( H = 16 \\), sequence \\( N = 2048 \\), \\( d_k = d_v = 64 \\)** (FP16):\n",
    "\\[\n",
    "\\text{Memory} = 4 \\times 16 \\times 2048 \\times 128 \\times 2 = 128MB\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **Shared KV Cache Memory Usage**\n",
    "With a **shared KV cache**, we **remove batch duplication**:\n",
    "\\[\n",
    "\\text{Memory} = H \\times N \\times (d_k + d_v) \\times 2\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "= 16 \\times 2048 \\times 128 \\times 2 = 32MB\n",
    "\\]\n",
    "\n",
    "‚úÖ **Saves 75% of memory!**\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Summary**\n",
    "| Feature | Standard KV Cache | Shared KV Cache |\n",
    "|---------|-----------------|----------------|\n",
    "| **Memory Usage** | \\( O(BHN) \\) | \\( O(HN) \\) (Much Lower) |\n",
    "| **Computation Speed** | Redundant KV recomputation | Faster (Reuses KV Cache) |\n",
    "| **Ideal for** | Single-sequence generation | Beam search, multi-turn chatbots |\n",
    "\n",
    "üöÄ **Shared KV Cache significantly reduces memory while speeding up decoding!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Windows Optimization for Large Language Models (LLMs)**\n",
    "Running LLMs efficiently on Windows is challenging due to hardware limitations like **VRAM, RAM, and CPU constraints**. Here are **three main optimization techniques**:\n",
    "\n",
    "---\n",
    "\n",
    "### **(1.1) StreamingLLM (ÊµÅÂºèÊé®ÁêÜ)**\n",
    "**StreamingLLM** is a technique that allows **partial processing of the KV cache** to handle long sequences efficiently **without excessive memory usage**.\n",
    "\n",
    "#### **How StreamingLLM Works:**\n",
    "- Instead of storing **all KV cache** in memory, **only the most relevant part** is kept in **GPU memory (VRAM)**.\n",
    "- Older keys and values are **offloaded** to slower storage (**CPU RAM or disk**) and loaded back when needed.\n",
    "- This allows the model to handle **longer sequences (e.g., 100K+ tokens)** without running out of memory.\n",
    "\n",
    "#### **Mathematical View of StreamingLLM**\n",
    "Normally, the **KV cache grows linearly** with sequence length \\( N \\):\n",
    "\n",
    "\\[\n",
    "\\text{Memory} = H \\times N \\times (d_k + d_v) \\times 2\n",
    "\\]\n",
    "\n",
    "For **very long sequences**, we **offload past KV pairs**:\n",
    "\\[\n",
    "K_{\\text{active}} = K_{\\text{cache}}[-M:]\n",
    "\\]\n",
    "\\[\n",
    "V_{\\text{active}} = V_{\\text{cache}}[-M:]\n",
    "\\]\n",
    "where \\( M \\) is a **moving window** (e.g., last 4K tokens) that fits in VRAM.\n",
    "\n",
    "‚úÖ **Reduces VRAM usage significantly**  \n",
    "‚úÖ **Allows infinite-length sequences** (as long as CPU RAM is available)  \n",
    "\n",
    "üöÄ **Example Models Using StreamingLLM:**  \n",
    "- **Mistral 7B with 100K context**  \n",
    "- **LLaMA 3 fine-tuned with FlashAttention 2**  \n",
    "\n",
    "---\n",
    "\n",
    "### **(1.2) FlashAttention for Windows**\n",
    "FlashAttention is a specialized **memory-efficient attention mechanism** that optimizes how GPU memory accesses the KV cache.\n",
    "\n",
    "Instead of computing the **entire softmax attention at once**, FlashAttention **splits computation into smaller chunks** that fit into GPU cache memory.\n",
    "\n",
    "\\[\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "\\]\n",
    "\n",
    "**Optimized by FlashAttention:**\n",
    "- **Rearranges memory access** to improve speed.\n",
    "- **Avoids redundant computations** of softmax normalization.\n",
    "- **Works well on consumer GPUs** (like RTX 4090).\n",
    "\n",
    "üöÄ **Example Libraries:**  \n",
    "- `xformers` (used in Stable Diffusion)  \n",
    "- `flash-attn` (used in LLaMA and GPT models)  \n",
    "\n",
    "---\n",
    "\n",
    "### **(1.3) Offloading for Low VRAM Windows Systems**\n",
    "For Windows users with **limited GPU VRAM**, offloading is key.\n",
    "\n",
    "- **CUDA + Paged Attention** ‚Üí Uses **GPU VRAM** + **CPU RAM** together.\n",
    "- **4-bit Quantization** ‚Üí Reduces memory size of model weights.\n",
    "- **GGUF Format** ‚Üí Optimized for **llama.cpp** (runs LLaMA models efficiently on CPUs).\n",
    "\n",
    "‚úÖ **Allows LLMs to run on laptops and consumer GPUs**  \n",
    "‚úÖ **Enables 65B models on 24GB VRAM GPUs (e.g., RTX 4090)**  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. ÈáèÂåñ (Quantization) ‰∏éÁ≥ªÊï∞ (Coefficient Reduction)**\n",
    "Quantization **reduces the bit precision** of model weights to **save memory and speed up inference**.\n",
    "\n",
    "---\n",
    "\n",
    "### **(2.1) What is Quantization?**\n",
    "In standard models:\n",
    "- **Weights use 16-bit (FP16) or 32-bit (FP32) floating-point numbers**.\n",
    "\n",
    "Quantization **reduces precision** to:\n",
    "- **8-bit (INT8)**\n",
    "- **4-bit (INT4)**\n",
    "- **3-bit or 2-bit (Extreme Quantization)**\n",
    "\n",
    "\\[\n",
    "W_{\\text{quantized}} = \\frac{W}{S} + Z\n",
    "\\]\n",
    "where:\n",
    "- \\( S \\) is a **scaling factor**.\n",
    "- \\( Z \\) is a **zero-point offset**.\n",
    "\n",
    "### **(2.2) Types of Quantization**\n",
    "| Type | Bit Precision | Speed Boost | Memory Reduction |\n",
    "|------|-------------|-------------|------------------|\n",
    "| **FP16 (Default)** | 16-bit | ‚úÖ Standard | ‚ùå High Memory |\n",
    "| **INT8 Quantization** | 8-bit | ‚ö° 1.5x Faster | ‚úÖ 2x Smaller |\n",
    "| **4-bit Quantization (GPTQ, AWQ)** | 4-bit | ‚ö°‚ö° 2-3x Faster | ‚úÖ‚úÖ 4x Smaller |\n",
    "| **2-bit Quantization** | 2-bit | ‚ö°‚ö°‚ö° 4-5x Faster | ‚úÖ‚úÖ‚úÖ 8x Smaller |\n",
    "\n",
    "üöÄ **Example Models Using Quantization:**\n",
    "- **LLaMA-3 7B (4-bit GGUF)**\n",
    "- **Mixtral (8-bit AWQ)**\n",
    "\n",
    "---\n",
    "\n",
    "### **(2.3) Á≥ªÊï∞‰ºòÂåñ (Coefficient Optimization)**\n",
    "Transformers rely on **large parameter matrices** (coefficients) to represent knowledge.\n",
    "\n",
    "- **Low-Rank Adaptation (LoRA)**  \n",
    "  - Instead of **fine-tuning all weights**, LoRA **trains only small rank-matrices \\( \\Delta W \\)**.\n",
    "  - Reduces **storage requirements** while keeping model accuracy.\n",
    "\n",
    "- **Weight Pruning**  \n",
    "  - Removes **less important weights** in neural networks.\n",
    "  - Example: **Sparsity-based pruning (LTH - Lottery Ticket Hypothesis)**.\n",
    "\n",
    "‚úÖ **Combining Quantization + LoRA reduces LLM memory needs by 10x!**  \n",
    "\n",
    "---\n",
    "\n",
    "## **3. Store & Computational Optimization**\n",
    "This includes techniques to **store and compute large models efficiently**.\n",
    "\n",
    "---\n",
    "\n",
    "### **(3.1) KV Cache Compression**\n",
    "Since KV Cache **grows with sequence length**, we can **compress it** to **save memory**.\n",
    "\n",
    "- **Low-Rank KV Cache** ‚Üí Stores **only important activations**.\n",
    "- **Grouped KV Storage** ‚Üí Merges similar key-value vectors.\n",
    "\n",
    "‚úÖ **Cuts KV memory usage by 50% without reducing accuracy!**  \n",
    "\n",
    "---\n",
    "\n",
    "### **(3.2) FlashInfer: Fast Matrix Multiplication**\n",
    "Large models require **fast tensor computation**.\n",
    "\n",
    "- FlashInfer **uses CUDA kernels** for efficient **matrix multiplication (MatMul)**.\n",
    "- Replaces **slow PyTorch ops** with **optimized NVIDIA operations**.\n",
    "\n",
    "‚úÖ **Speeds up inference by 2-4x on RTX 4090!**  \n",
    "\n",
    "---\n",
    "\n",
    "### **(3.3) Checkpointing & Gradient Offloading**\n",
    "For **training large models**:\n",
    "- **Gradient Offloading** ‚Üí Moves gradients to CPU RAM.\n",
    "- **Activation Checkpointing** ‚Üí Only recomputes essential activations.\n",
    "\n",
    "‚úÖ **Allows training LLaMA-3 65B on 48GB GPUs!**  \n",
    "\n",
    "\n",
    "### **(3.4) FlashDecoding and VLLM Pageattention**\n",
    "For **training large models**:\n",
    "- **flashdecoding make kv in chunk and reduce\n",
    "- **store kv in not connected meomory space.\n",
    "\n",
    "‚úÖ **Allows training LLaMA-3 in faster way!**  \n",
    "\n",
    "---\n",
    "\n",
    "## **Final Summary**\n",
    "| **Optimization Type** | **Technique** | **Benefit** |\n",
    "|------------------|----------------|------------|\n",
    "| **Windows Optimizations** | StreamingLLM, FlashAttention, Offloading | Run LLMs on low VRAM GPUs |\n",
    "| **Quantization (ÈáèÂåñ)** | 8-bit, 4-bit, LoRA | Reduce memory usage 4-10x |\n",
    "| **Storage Optimization** | KV Cache Compression, Checkpointing | Lower GPU memory needs |\n",
    "| **Compute Optimization** | FlashInfer, CUDA Kernels | Speed up inference |\n",
    "\n",
    "üöÄ **Using these techniques, you can run massive LLMs (e.g., 65B models) efficiently on consumer hardware!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
