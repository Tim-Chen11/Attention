{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **KV Cache in Transformer Decoders**\n",
    "### **1. What is KV Cache?**\n",
    "The **KV Cache** (Key-Value Cache) is an optimization technique used in **autoregressive decoding** to **reuse previously computed key (K) and value (V) matrices** instead of recomputing them at every decoding step. This significantly reduces **computational cost** and **memory usage** in large transformer models like **GPT, LLaMA, and ChatGPT**.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Why Do We Need KV Cache?**\n",
    "### **The Problem in Standard Decoding**\n",
    "In an **autoregressive transformer decoder**, each token in the sequence is generated **one-by-one**, and at every step \\( t \\), we compute:\n",
    "\n",
    "1. **Query (Q), Key (K), and Value (V)** matrices:\n",
    "   \\[\n",
    "   Q_t = X_t W_Q, \\quad K_t = X_t W_K, \\quad V_t = X_t W_V\n",
    "   \\]\n",
    "2. Compute **self-attention**:\n",
    "   \\[\n",
    "   A_t = \\text{softmax} \\left( \\frac{Q_t K^T}{\\sqrt{d_k}} \\right) V\n",
    "   \\]\n",
    "\n",
    "At each step \\( t \\), the decoder needs to **recompute** all previous keys and values to perform attention, leading to **quadratic complexity** \\( O(N^2 d) \\).\n",
    "\n",
    "---\n",
    "### **How KV Cache Fixes This**\n",
    "Instead of recomputing **all** keys and values from scratch at every step, we **store the past keys and values** and only append new ones.\n",
    "\n",
    "1. **Store Previous \\( K, V \\) in Memory**:\n",
    "   \\[\n",
    "   K_{\\text{cache}} = [K_1, K_2, ..., K_{t-1}]\n",
    "   \\]\n",
    "   \\[\n",
    "   V_{\\text{cache}} = [V_1, V_2, ..., V_{t-1}]\n",
    "   \\]\n",
    "2. **At Step \\( t \\), Only Compute for New Token**:\n",
    "   - Compute **only \\( K_t \\) and \\( V_t \\)**.\n",
    "   - Append them to the cache:\n",
    "     \\[\n",
    "     K_{\\text{cache}} = [K_{\\text{cache}}, K_t]\n",
    "     \\]\n",
    "     \\[\n",
    "     V_{\\text{cache}} = [V_{\\text{cache}}, V_t]\n",
    "     \\]\n",
    "3. **Compute Attention Efficiently**:\n",
    "   \\[\n",
    "   A_t = \\text{softmax} \\left( \\frac{Q_t K_{\\text{cache}}^T}{\\sqrt{d_k}} \\right) V_{\\text{cache}}\n",
    "   \\]\n",
    "\n",
    "Now, instead of recomputing all previous key-value pairs, the model **only updates the new token's values and performs attention efficiently**.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Mathematical Formulation of KV Cache**\n",
    "### **Without KV Cache (Recomputing Each Step)**\n",
    "At every step \\( t \\), standard self-attention requires:\n",
    "\\[\n",
    "A_t = \\text{softmax} \\left( \\frac{Q_t K^T}{\\sqrt{d_k}} \\right) V\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( Q_t \\) is the query for the current token.\n",
    "- \\( K = [K_1, ..., K_t] \\) (computed fresh at every step).\n",
    "- \\( V = [V_1, ..., V_t] \\).\n",
    "\n",
    "This leads to \\( O(N^2 d) \\) complexity.\n",
    "\n",
    "---\n",
    "\n",
    "### **With KV Cache (Efficient Computation)**\n",
    "Instead of recalculating old \\( K \\) and \\( V \\), we store:\n",
    "\n",
    "\\[\n",
    "K_{\\text{cache}} = [K_1, K_2, ..., K_{t-1}]\n",
    "\\]\n",
    "\\[\n",
    "V_{\\text{cache}} = [V_1, V_2, ..., V_{t-1}]\n",
    "\\]\n",
    "\n",
    "Then, at **step \\( t \\)**:\n",
    "1. Compute only **new key and value**:\n",
    "   \\[\n",
    "   K_t = X_t W_K, \\quad V_t = X_t W_V\n",
    "   \\]\n",
    "2. Append to the cache:\n",
    "   \\[\n",
    "   K_{\\text{cache}} = [K_{\\text{cache}}, K_t], \\quad V_{\\text{cache}} = [V_{\\text{cache}}, V_t]\n",
    "   \\]\n",
    "3. Compute **attention only on stored values**:\n",
    "   \\[\n",
    "   A_t = \\text{softmax} \\left( \\frac{Q_t K_{\\text{cache}}^T}{\\sqrt{d_k}} \\right) V_{\\text{cache}}\n",
    "   \\]\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Memory Usage of KV Cache**\n",
    "Since we store all previous keys and values, memory usage **grows linearly** with sequence length \\( N \\).\n",
    "\n",
    "### **Memory Calculation**\n",
    "Each token stores:\n",
    "\\[\n",
    "K_t \\in \\mathbb{R}^{d_k}, \\quad V_t \\in \\mathbb{R}^{d_v}\n",
    "\\]\n",
    "\n",
    "For a **batch size \\( B \\)**, **number of heads \\( H \\)**, and **sequence length \\( N \\)**:\n",
    "- \\( K_{\\text{cache}} \\) has shape \\( (B, H, N, d_k) \\).\n",
    "- \\( V_{\\text{cache}} \\) has shape \\( (B, H, N, d_v) \\).\n",
    "\n",
    "Thus, the **total KV cache memory** is:\n",
    "\\[\n",
    "\\text{Memory} = B \\times H \\times N \\times (d_k + d_v) \\times \\text{data type size}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **Example: GPT-4 Memory Usage**\n",
    "Assume:\n",
    "- Batch size **\\( B = 4 \\)**\n",
    "- Heads **\\( H = 16 \\)**\n",
    "- Sequence length **\\( N = 2048 \\)**\n",
    "- Key/Value size **\\( d_k = d_v = 64 \\)**\n",
    "- Data type **FP16 (2 bytes per element)**\n",
    "\n",
    "#### **Compute Memory for KV Cache**\n",
    "\\[\n",
    "\\text{Memory} = 4 \\times 16 \\times 2048 \\times (64 + 64) \\times 2\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "= 4 \\times 16 \\times 2048 \\times 128 \\times 2\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "= 4 \\times 16 \\times 524,288\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "= 33,554,432 \\text{ bytes} = 32MB\n",
    "\\]\n",
    "\n",
    "For **longer sequences (e.g., 8K tokens)**, memory grows **linearly**.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Advantages of KV Cache**\n",
    "âœ… **Speeds Up Decoding** â†’ Avoids recomputation, reducing latency.  \n",
    "âœ… **Reduces Computational Cost** â†’ No need to multiply against all past tokens.  \n",
    "âœ… **Efficient for Large Models** â†’ Used in **GPT-3, GPT-4, LLaMA, ChatGPT**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **6. KV Cache vs. No KV Cache**\n",
    "| Feature | Without KV Cache | With KV Cache |\n",
    "|---------|-----------------|--------------|\n",
    "| **Computation** | \\( O(N^2 d) \\) | \\( O(N d) \\) |\n",
    "| **Memory Usage** | Lower | Higher (stores all past K, V) |\n",
    "| **Decoding Speed** | Slow | Fast |\n",
    "| **Used In** | RNNs, Small Transformers | GPT-3, LLaMA, ChatGPT |\n",
    "\n",
    "ðŸš€ **KV Caching makes large-scale transformers efficient for text generation!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **KV Cache Optimization: Shared KV Cache (å…±ç”¨ KV Cache)**\n",
    "Using a **shared KV cache (å…±ç”¨ KV Cache)** is an optimization strategy that allows multiple queries (Q) to use the same precomputed **key (K) and value (V) matrices**, reducing redundant computations and improving efficiency in transformer decoders.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. What is Shared KV Cache?**\n",
    "### **Standard KV Cache vs. Shared KV Cache**\n",
    "- **Standard KV Cache**: Each sequence in a batch has its own independent **K and V** cache.\n",
    "- **Shared KV Cache**: Multiple sequences **reuse the same** precomputed **K, V** cache to **reduce memory usage**.\n",
    "\n",
    "### **Why Use Shared KV Cache?**\n",
    "1. **Lower Memory Footprint**  \n",
    "   - Instead of storing separate KV caches for each sequence, we **share one cache** across multiple decoder steps or sequences.\n",
    "  \n",
    "2. **Faster Decoding**  \n",
    "   - Since the **same K and V** are used across different queries, we avoid recomputing keys/values for every new sequence.\n",
    "\n",
    "3. **Ideal for Multi-Turn Chatbots & Beam Search**  \n",
    "   - When generating multiple responses for the same prompt, shared KV cache allows efficient reuse of **previous K, V states**.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Mathematical Explanation of Shared KV Cache**\n",
    "### **Standard KV Cache (Without Sharing)**\n",
    "For a sequence of length \\( N \\), at each decoding step \\( t \\):\n",
    "\n",
    "1. Compute **new K and V**:\n",
    "   \\[\n",
    "   K_t = X_t W_K, \\quad V_t = X_t W_V\n",
    "   \\]\n",
    "\n",
    "2. Append to KV Cache:\n",
    "   \\[\n",
    "   K_{\\text{cache}} = [K_{\\text{cache}}, K_t], \\quad V_{\\text{cache}} = [V_{\\text{cache}}, V_t]\n",
    "   \\]\n",
    "\n",
    "3. Compute Attention:\n",
    "   \\[\n",
    "   A_t = \\text{softmax} \\left( \\frac{Q_t K_{\\text{cache}}^T}{\\sqrt{d_k}} \\right) V_{\\text{cache}}\n",
    "   \\]\n",
    "\n",
    "This is done **independently** for each sequence, leading to redundant storage of similar \\( K \\) and \\( V \\).\n",
    "\n",
    "---\n",
    "\n",
    "### **Shared KV Cache (Optimized)**\n",
    "Instead of storing separate KV caches, we **reuse one shared cache** across multiple sequences or decoding steps.\n",
    "\n",
    "1. Store **only one copy** of \\( K, V \\) for multiple sequences:\n",
    "   \\[\n",
    "   K_{\\text{shared}} = K_{\\text{cache}}, \\quad V_{\\text{shared}} = V_{\\text{cache}}\n",
    "   \\]\n",
    "\n",
    "2. Compute attention **using the shared cache**:\n",
    "   \\[\n",
    "   A_t = \\text{softmax} \\left( \\frac{Q_t K_{\\text{shared}}^T}{\\sqrt{d_k}} \\right) V_{\\text{shared}}\n",
    "   \\]\n",
    "\n",
    "3. No need to **duplicate K and V** for each sequence.\n",
    "\n",
    "This reduces **memory overhead from \\( O(BHN) \\) to \\( O(HN) \\)**, where:\n",
    "- \\( B \\) = batch size\n",
    "- \\( H \\) = number of attention heads\n",
    "- \\( N \\) = sequence length\n",
    "\n",
    "---\n",
    "\n",
    "## **3. When to Use Shared KV Cache?**\n",
    "### **Use Case 1: Beam Search**\n",
    "- In **beam search**, we generate multiple sequences **from the same prompt**.\n",
    "- Instead of storing **separate KV caches for each beam**, we **share one KV cache** across all beams.\n",
    "  \n",
    "### **Use Case 2: Multi-Turn Conversations**\n",
    "- When responding to **multiple follow-up queries** in a chatbot, we can **reuse** the previous KV cache instead of recomputing it.\n",
    "  \n",
    "### **Use Case 3: Parallel Decoding in Multi-Agent Models**\n",
    "- In **multi-agent transformer systems**, different agents may **reference the same past context**, making shared KV cache **efficient**.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Memory Comparison: Standard vs. Shared KV Cache**\n",
    "### **Standard KV Cache Memory Usage**\n",
    "Each sequence stores **independent** KV caches:\n",
    "\n",
    "\\[\n",
    "\\text{Memory} = B \\times H \\times N \\times (d_k + d_v) \\times 2\n",
    "\\]\n",
    "\n",
    "For **batch size \\( B = 4 \\), heads \\( H = 16 \\), sequence \\( N = 2048 \\), \\( d_k = d_v = 64 \\)** (FP16):\n",
    "\\[\n",
    "\\text{Memory} = 4 \\times 16 \\times 2048 \\times 128 \\times 2 = 128MB\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **Shared KV Cache Memory Usage**\n",
    "With a **shared KV cache**, we **remove batch duplication**:\n",
    "\\[\n",
    "\\text{Memory} = H \\times N \\times (d_k + d_v) \\times 2\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "= 16 \\times 2048 \\times 128 \\times 2 = 32MB\n",
    "\\]\n",
    "\n",
    "âœ… **Saves 75% of memory!**\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Summary**\n",
    "| Feature | Standard KV Cache | Shared KV Cache |\n",
    "|---------|-----------------|----------------|\n",
    "| **Memory Usage** | \\( O(BHN) \\) | \\( O(HN) \\) (Much Lower) |\n",
    "| **Computation Speed** | Redundant KV recomputation | Faster (Reuses KV Cache) |\n",
    "| **Ideal for** | Single-sequence generation | Beam search, multi-turn chatbots |\n",
    "\n",
    "ðŸš€ **Shared KV Cache significantly reduces memory while speeding up decoding!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
