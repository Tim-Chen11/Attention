{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **KV Cache in Transformer Decoders**\n",
    "### **1. What is KV Cache?**\n",
    "The **KV Cache** (Key-Value Cache) is an optimization technique used in **autoregressive decoding** to **reuse previously computed key (K) and value (V) matrices** instead of recomputing them at every decoding step. This significantly reduces **computational cost** and **memory usage** in large transformer models like **GPT, LLaMA, and ChatGPT**.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Why Do We Need KV Cache?**\n",
    "### **The Problem in Standard Decoding**\n",
    "In an **autoregressive transformer decoder**, each token in the sequence is generated **one-by-one**, and at every step \\( t \\), we compute:\n",
    "\n",
    "1. **Query (Q), Key (K), and Value (V)** matrices:\n",
    "   \\[\n",
    "   Q_t = X_t W_Q, \\quad K_t = X_t W_K, \\quad V_t = X_t W_V\n",
    "   \\]\n",
    "2. Compute **self-attention**:\n",
    "   \\[\n",
    "   A_t = \\text{softmax} \\left( \\frac{Q_t K^T}{\\sqrt{d_k}} \\right) V\n",
    "   \\]\n",
    "\n",
    "At each step \\( t \\), the decoder needs to **recompute** all previous keys and values to perform attention, leading to **quadratic complexity** \\( O(N^2 d) \\).\n",
    "\n",
    "---\n",
    "### **How KV Cache Fixes This**\n",
    "Instead of recomputing **all** keys and values from scratch at every step, we **store the past keys and values** and only append new ones.\n",
    "\n",
    "1. **Store Previous \\( K, V \\) in Memory**:\n",
    "   \\[\n",
    "   K_{\\text{cache}} = [K_1, K_2, ..., K_{t-1}]\n",
    "   \\]\n",
    "   \\[\n",
    "   V_{\\text{cache}} = [V_1, V_2, ..., V_{t-1}]\n",
    "   \\]\n",
    "2. **At Step \\( t \\), Only Compute for New Token**:\n",
    "   - Compute **only \\( K_t \\) and \\( V_t \\)**.\n",
    "   - Append them to the cache:\n",
    "     \\[\n",
    "     K_{\\text{cache}} = [K_{\\text{cache}}, K_t]\n",
    "     \\]\n",
    "     \\[\n",
    "     V_{\\text{cache}} = [V_{\\text{cache}}, V_t]\n",
    "     \\]\n",
    "3. **Compute Attention Efficiently**:\n",
    "   \\[\n",
    "   A_t = \\text{softmax} \\left( \\frac{Q_t K_{\\text{cache}}^T}{\\sqrt{d_k}} \\right) V_{\\text{cache}}\n",
    "   \\]\n",
    "\n",
    "Now, instead of recomputing all previous key-value pairs, the model **only updates the new token's values and performs attention efficiently**.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Mathematical Formulation of KV Cache**\n",
    "### **Without KV Cache (Recomputing Each Step)**\n",
    "At every step \\( t \\), standard self-attention requires:\n",
    "\\[\n",
    "A_t = \\text{softmax} \\left( \\frac{Q_t K^T}{\\sqrt{d_k}} \\right) V\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( Q_t \\) is the query for the current token.\n",
    "- \\( K = [K_1, ..., K_t] \\) (computed fresh at every step).\n",
    "- \\( V = [V_1, ..., V_t] \\).\n",
    "\n",
    "This leads to \\( O(N^2 d) \\) complexity.\n",
    "\n",
    "---\n",
    "\n",
    "### **With KV Cache (Efficient Computation)**\n",
    "Instead of recalculating old \\( K \\) and \\( V \\), we store:\n",
    "\n",
    "\\[\n",
    "K_{\\text{cache}} = [K_1, K_2, ..., K_{t-1}]\n",
    "\\]\n",
    "\\[\n",
    "V_{\\text{cache}} = [V_1, V_2, ..., V_{t-1}]\n",
    "\\]\n",
    "\n",
    "Then, at **step \\( t \\)**:\n",
    "1. Compute only **new key and value**:\n",
    "   \\[\n",
    "   K_t = X_t W_K, \\quad V_t = X_t W_V\n",
    "   \\]\n",
    "2. Append to the cache:\n",
    "   \\[\n",
    "   K_{\\text{cache}} = [K_{\\text{cache}}, K_t], \\quad V_{\\text{cache}} = [V_{\\text{cache}}, V_t]\n",
    "   \\]\n",
    "3. Compute **attention only on stored values**:\n",
    "   \\[\n",
    "   A_t = \\text{softmax} \\left( \\frac{Q_t K_{\\text{cache}}^T}{\\sqrt{d_k}} \\right) V_{\\text{cache}}\n",
    "   \\]\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Memory Usage of KV Cache**\n",
    "Since we store all previous keys and values, memory usage **grows linearly** with sequence length \\( N \\).\n",
    "\n",
    "### **Memory Calculation**\n",
    "Each token stores:\n",
    "\\[\n",
    "K_t \\in \\mathbb{R}^{d_k}, \\quad V_t \\in \\mathbb{R}^{d_v}\n",
    "\\]\n",
    "\n",
    "For a **batch size \\( B \\)**, **number of heads \\( H \\)**, and **sequence length \\( N \\)**:\n",
    "- \\( K_{\\text{cache}} \\) has shape \\( (B, H, N, d_k) \\).\n",
    "- \\( V_{\\text{cache}} \\) has shape \\( (B, H, N, d_v) \\).\n",
    "\n",
    "Thus, the **total KV cache memory** is:\n",
    "\\[\n",
    "\\text{Memory} = B \\times H \\times N \\times (d_k + d_v) \\times \\text{data type size}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **Example: GPT-4 Memory Usage**\n",
    "Assume:\n",
    "- Batch size **\\( B = 4 \\)**\n",
    "- Heads **\\( H = 16 \\)**\n",
    "- Sequence length **\\( N = 2048 \\)**\n",
    "- Key/Value size **\\( d_k = d_v = 64 \\)**\n",
    "- Data type **FP16 (2 bytes per element)**\n",
    "\n",
    "#### **Compute Memory for KV Cache**\n",
    "\\[\n",
    "\\text{Memory} = 4 \\times 16 \\times 2048 \\times (64 + 64) \\times 2\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "= 4 \\times 16 \\times 2048 \\times 128 \\times 2\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "= 4 \\times 16 \\times 524,288\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "= 33,554,432 \\text{ bytes} = 32MB\n",
    "\\]\n",
    "\n",
    "For **longer sequences (e.g., 8K tokens)**, memory grows **linearly**.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Advantages of KV Cache**\n",
    "âœ… **Speeds Up Decoding** â†’ Avoids recomputation, reducing latency.  \n",
    "âœ… **Reduces Computational Cost** â†’ No need to multiply against all past tokens.  \n",
    "âœ… **Efficient for Large Models** â†’ Used in **GPT-3, GPT-4, LLaMA, ChatGPT**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **6. KV Cache vs. No KV Cache**\n",
    "| Feature | Without KV Cache | With KV Cache |\n",
    "|---------|-----------------|--------------|\n",
    "| **Computation** | \\( O(N^2 d) \\) | \\( O(N d) \\) |\n",
    "| **Memory Usage** | Lower | Higher (stores all past K, V) |\n",
    "| **Decoding Speed** | Slow | Fast |\n",
    "| **Used In** | RNNs, Small Transformers | GPT-3, LLaMA, ChatGPT |\n",
    "\n",
    "ðŸš€ **KV Caching makes large-scale transformers efficient for text generation!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
