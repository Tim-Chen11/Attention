{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Understanding Attention Mechanism in Deep Learning\n", "\n", "In this notebook, we will explore the concept of attention mechanisms in deep learning, particularly focusing on self-attention, scaled dot-product attention, multi-head attention, and masking. The attention mechanism is a fundamental component of modern architectures like the Transformer, which powers models such as BERT and GPT.\n", "\n", "We will cover:\n", "- **Query, Key, and Value (QKV) Representation**\n", "- **Attention Formula and Intuition**\n", "- **Scaling and Scaled Dot-Product Attention**\n", "- **Multi-Head Attention**\n", "- **Masking Techniques in Attention**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Query, Key, and Value (QKV)\n", "\n", "The attention mechanism works by computing a weighted sum of values based on the similarity between queries and keys.\n", "- **Query (Q):** The input vector for which we want to compute attention.\n", "- **Key (K):** The reference vectors against which the query is compared.\n", "- **Value (V):** The actual content vectors that are aggregated based on attention scores.\n", "\n", "Each token in the input sequence is transformed into Q, K, and V using learnable weight matrices."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "import torch.nn.functional as F\n", "\n", "# Define input tensor (batch_size=1, seq_length=3, embedding_dim=4)\n", "input_tensor = torch.randn(1, 3, 4)\n", "\n", "# Define weight matrices for Q, K, V\n", "W_q = torch.randn(4, 4)\n", "W_k = torch.randn(4, 4)\n", "W_v = torch.randn(4, 4)\n", "\n", "# Compute Q, K, V\n", "Q = input_tensor @ W_q\n", "K = input_tensor @ W_k\n", "V = input_tensor @ W_v\n", "\n", "print(\"Query (Q):\", Q)\n", "print(\"Key (K):\", K)\n", "print(\"Value (V):\", V)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Attention Formula\n", "\n", "The core of the attention mechanism is:\n", "\\[\n", "   \\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n", "\\]\n", "\n", "Where:\n", "- \\( QK^T \\) computes similarity scores between queries and keys.\n", "- Scaling by \\( \\sqrt{d_k} \\) prevents large dot-product values from leading to small gradients.\n", "- Softmax normalizes the scores into probabilities.\n", "- These probabilities weight the values (V)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Compute attention scores\n", "dk = Q.shape[-1]\n", "attention_scores = (Q @ K.transpose(-2, -1)) / torch.sqrt(torch.tensor(dk, dtype=torch.float32))\n", "attention_weights = F.softmax(attention_scores, dim=-1)\n", "\n", "# Compute attention output\n", "attention_output = attention_weights @ V\n", "\n", "print(\"Attention Weights:\", attention_weights)\n", "print(\"Attention Output:\", attention_output)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Scaled Dot-Product Attention\n", "The scaled dot-product attention is the operation defined above. The division by \\( \\sqrt{d_k} \\) is critical for stabilizing training."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Multi-Head Attention\n", "Instead of using a single attention function, **multi-head attention** employs multiple attention heads to learn different aspects of the input representation.\n", "\n", "Each head has independent weight matrices for Q, K, and V. The outputs from all heads are concatenated and linearly transformed."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Example of Multi-Head Attention with 2 heads\n", "num_heads = 2\n", "head_dim = dk // num_heads\n", "\n", "# Splitting Q, K, V into multiple heads\n", "Q_heads = Q.view(1, 3, num_heads, head_dim).transpose(1, 2)\n", "K_heads = K.view(1, 3, num_heads, head_dim).transpose(1, 2)\n", "V_heads = V.view(1, 3, num_heads, head_dim).transpose(1, 2)\n", "\n", "# Compute attention per head\n", "attention_scores_heads = (Q_heads @ K_heads.transpose(-2, -1)) / torch.sqrt(torch.tensor(head_dim, dtype=torch.float32))\n", "attention_weights_heads = F.softmax(attention_scores_heads, dim=-1)\n", "attention_output_heads = attention_weights_heads @ V_heads\n", "\n", "# Concatenate heads\n", "multi_head_output = attention_output_heads.transpose(1, 2).reshape(1, 3, dk)\n", "\n", "print(\"Multi-Head Attention Output:\", multi_head_output)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5. Masking in Attention\n", "\n", "There are two main types of masks used in attention mechanisms:\n", "- **Padding Mask:** Ensures that padding tokens do not influence the attention scores.\n", "- **Lookahead Mask:** Used in autoregressive models (e.g., GPT) to prevent attention to future tokens.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Example of Masking\n", "mask = torch.tensor([[1, 0, 0], [1, 1, 0], [1, 1, 1]])  # Upper-triangular mask\n", "masked_attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n", "masked_attention_weights = F.softmax(masked_attention_scores, dim=-1)\n", "\n", "print(\"Masked Attention Weights:\", masked_attention_weights)"]}], "metadata": {}, "nbformat": 4, "nbformat_minor": 4}