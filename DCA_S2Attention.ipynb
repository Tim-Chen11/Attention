{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dual Chunk Attention (DCA)**\n",
    "### **1. What is Dual Chunk Attention?**\n",
    "**Dual Chunk Attention (DCA)** is an efficient attention mechanism designed to handle **long sequences** by **splitting input sequences into smaller \"chunks\"** and applying attention within and across these chunks. It helps **reduce computational complexity** while retaining **global and local dependencies**.\n",
    "\n",
    "üîπ **Key Idea:** Instead of computing full self-attention over an entire sequence, DCA **divides** the sequence into chunks and applies attention in **two steps**:\n",
    "1. **Intra-Chunk Attention** ‚Üí Focuses on relationships **within each chunk**.\n",
    "2. **Inter-Chunk Attention** ‚Üí Captures dependencies **between different chunks**.\n",
    "\n",
    "By structuring attention in this way, DCA can **scale better for long sequences** compared to traditional self-attention.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Why Use Dual Chunk Attention?**\n",
    "Standard **self-attention** has **quadratic complexity**:  \n",
    "\\[\n",
    "O(N^2 d)\n",
    "\\]\n",
    "where:\n",
    "- \\( N \\) = Sequence length\n",
    "- \\( d \\) = Embedding dimension\n",
    "\n",
    "For **long sequences (e.g., 100K+ tokens)**, this becomes **computationally expensive** and **memory-intensive**.\n",
    "\n",
    "**DCA solves this by**:\n",
    "‚úÖ **Reducing computational complexity** from \\( O(N^2) \\) to **\\( O(C^2 + C^2) \\)** where \\( C \\) is chunk size.  \n",
    "‚úÖ **Maintaining local and global dependencies** efficiently.  \n",
    "‚úÖ **Allowing longer sequences to fit in GPU memory**.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. How Does Dual Chunk Attention Work?**\n",
    "DCA **divides** the sequence into multiple chunks and applies **attention in two phases**.\n",
    "\n",
    "### **Step 1: Divide the Sequence into Chunks**\n",
    "Given an input sequence **\\( X \\)** of length \\( N \\), divide it into **\\( K \\) chunks**, each of size \\( C \\):\n",
    "\\[\n",
    "X = [X_1, X_2, ..., X_K]\n",
    "\\]\n",
    "where each chunk **\\( X_i \\in \\mathbb{R}^{C \\times d} \\)**.\n",
    "\n",
    "### **Step 2: Apply Intra-Chunk Attention**\n",
    "Within each chunk, we apply **standard self-attention**:\n",
    "\\[\n",
    "A_{\\text{intra}}^i = \\text{softmax} \\left( \\frac{Q_i K_i^T}{\\sqrt{d_k}} \\right) V_i\n",
    "\\]\n",
    "where:\n",
    "- \\( Q_i, K_i, V_i \\) are query, key, and value matrices for chunk \\( i \\).\n",
    "- This **captures local relationships** inside each chunk.\n",
    "\n",
    "üöÄ **Complexity for intra-chunk attention:**\n",
    "\\[\n",
    "O(K C^2) = O(N C)\n",
    "\\]\n",
    "since **\\( K = N / C \\)**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Apply Inter-Chunk Attention**\n",
    "To capture **long-range dependencies**, we compute **attention across different chunks**.\n",
    "\n",
    "For each chunk \\( i \\), we compute:\n",
    "\\[\n",
    "A_{\\text{inter}}^i = \\text{softmax} \\left( \\frac{Q_i K_{\\text{global}}^T}{\\sqrt{d_k}} \\right) V_{\\text{global}}\n",
    "\\]\n",
    "where:\n",
    "- \\( K_{\\text{global}} \\) and \\( V_{\\text{global}} \\) represent **summarized embeddings** of other chunks.\n",
    "- This allows information to **flow between distant tokens**.\n",
    "\n",
    "üöÄ **Complexity for inter-chunk attention:**\n",
    "\\[\n",
    "O(K^2 C) = O(N)\n",
    "\\]\n",
    "since each chunk interacts **only with global representations**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Combine Both Attention Outputs**\n",
    "The final attention output combines both **intra-chunk** and **inter-chunk** attention:\n",
    "\\[\n",
    "A_{\\text{final}}^i = A_{\\text{intra}}^i + A_{\\text{inter}}^i\n",
    "\\]\n",
    "which ensures both **local token relationships** and **long-distance dependencies** are captured.\n",
    "\n",
    "üöÄ **Final Complexity of Dual Chunk Attention:**\n",
    "\\[\n",
    "O(NC) + O(N) = O(NC)\n",
    "\\]\n",
    "which is **significantly more efficient** than standard self-attention \\( O(N^2) \\).\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Pros and Cons of Dual Chunk Attention**\n",
    "### ‚úÖ **Advantages**\n",
    "- **Reduces memory usage** ‚Üí Suitable for **long documents and sequences**.\n",
    "- **Maintains local and global dependencies** ‚Üí Unlike naive **windowed attention** (e.g., Longformer).\n",
    "- **Efficient for real-time processing** ‚Üí Works well for **speech, music, and large-scale NLP**.\n",
    "\n",
    "### ‚ùå **Disadvantages**\n",
    "- **Needs careful chunk size selection** ‚Üí Too small can hurt long-range understanding, too large increases memory.\n",
    "- **More complex implementation** ‚Üí Requires additional mechanisms to **summarize global chunks**.\n",
    "\n",
    "üöÄ **Example Models Using Dual Chunk Attention:**\n",
    "- **MegaByte Transformer (Google DeepMind)**\n",
    "- **Efficient Transformers for 1M-token sequences**\n",
    "- **Long-range Transformers for Speech Recognition**\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Comparison with Other Attention Mechanisms**\n",
    "| **Method** | **Complexity** | **Captures Local Dependencies?** | **Captures Global Dependencies?** |\n",
    "|------------|--------------|--------------------------------|--------------------------------|\n",
    "| **Standard Self-Attention** | \\( O(N^2) \\) | ‚úÖ Yes | ‚úÖ Yes |\n",
    "| **Sliding Window Attention (Longformer)** | \\( O(NC) \\) | ‚úÖ Yes | ‚ùå No |\n",
    "| **Sparse Attention (BigBird)** | \\( O(N \\log N) \\) | ‚úÖ Yes | ‚úÖ Yes (partially) |\n",
    "| **Grouped-Query Attention (GQA)** | \\( O(N) \\) | ‚ùå No | ‚úÖ Yes |\n",
    "| **Dual Chunk Attention (DCA)** | \\( O(NC) \\) | ‚úÖ Yes | ‚úÖ Yes |\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Final Summary**\n",
    "- **Dual Chunk Attention (DCA) splits sequences into chunks** to balance **efficiency and accuracy**.\n",
    "- **Two-step attention process:**\n",
    "  - **Intra-chunk attention** (local token relations).\n",
    "  - **Inter-chunk attention** (global context across chunks).\n",
    "- **Reduces memory and computational cost** while maintaining **long-range dependencies**.\n",
    "\n",
    "üöÄ **DCA is a powerful method for handling extremely long sequences in AI models!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Shifted Sparse Attention (SSA)**\n",
    "### **1. What is Shifted Sparse Attention?**\n",
    "**Shifted Sparse Attention (SSA)** is an **optimized attention mechanism** that reduces the **computational cost** of self-attention while still capturing **long-range dependencies**. \n",
    "\n",
    "üí° **Key Idea:**  \n",
    "- Instead of computing **full self-attention**, SSA **selectively attends to a subset of tokens** using **sparse patterns**.\n",
    "- To ensure **full coverage of dependencies**, **different attention heads are ‚Äúshifted‚Äù** to focus on **different token subsets**.\n",
    "- This improves **model efficiency** while maintaining **global context awareness**.\n",
    "\n",
    "üöÄ **Example:** SSA is used in **Swin Transformers (vision models)** and **Efficient NLP Transformers**.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Why Do We Need Shifted Sparse Attention?**\n",
    "### **Problem with Standard Self-Attention**\n",
    "- **Full self-attention has \\( O(N^2) \\) complexity**, making it computationally expensive for long sequences.\n",
    "- Large models (e.g., **GPT-4, BERT**) require huge amounts of **memory and compute power**.\n",
    "\n",
    "### **Sparse Attention: A Partial Solution**\n",
    "Sparse attention mechanisms (e.g., **Longformer, BigBird**) **reduce computation** by limiting attention to **specific token subsets**.\n",
    "\n",
    "### **Problem with Basic Sparse Attention**\n",
    "- If we **only use fixed sparse patterns**, some **tokens may never attend to each other**.\n",
    "- **Important long-range dependencies** might be lost.\n",
    "\n",
    "### **Solution: Shifted Sparse Attention**\n",
    "üí° **SSA combines sparse attention with a shifting mechanism**:\n",
    "1. **Each head attends to a sparse pattern** (e.g., every 3rd token).\n",
    "2. **Attention windows are ‚Äúshifted‚Äù** in different heads to cover **more positions**.\n",
    "3. This ensures that **all tokens get attended to at least once**.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. How Does Shifted Sparse Attention Work?**\n",
    "### **Step 1: Define Sparse Attention Pattern**\n",
    "Instead of computing full attention:\n",
    "\\[\n",
    "A = \\text{softmax} \\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right) V\n",
    "\\]\n",
    "SSA **restricts attention** to only specific tokens.\n",
    "\n",
    "For example:\n",
    "- **Sliding Window Attention (Longformer)** attends to nearby tokens **within a fixed range**.\n",
    "- **Block Sparse Attention (BigBird)** attends to **fixed interval tokens**.\n",
    "\n",
    "In SSA, each token only attends to:\n",
    "1. **Local window neighbors** (like Longformer).\n",
    "2. **Some preselected distant tokens** (like BigBird).\n",
    "3. **A shifted sparse pattern across heads**.\n",
    "\n",
    "### **Step 2: Shift Attention Windows Across Heads**\n",
    "To ensure all tokens interact at some point:\n",
    "- **Each head uses a slightly different offset** in its sparse pattern.\n",
    "- **Different heads focus on different token subsets**.\n",
    "  \n",
    "For example:\n",
    "| **Head** | **Sparse Pattern** |\n",
    "|----------|-------------------|\n",
    "| **Head 1** | Attends to every **3rd token** (0, 3, 6, 9, ‚Ä¶) |\n",
    "| **Head 2** | Shifted by 1 ‚Üí Attends to (1, 4, 7, 10, ‚Ä¶) |\n",
    "| **Head 3** | Shifted by 2 ‚Üí Attends to (2, 5, 8, 11, ‚Ä¶) |\n",
    "\n",
    "This ensures that **every token gets attended to at least once**.\n",
    "\n",
    "### **Step 3: Compute Shifted Sparse Attention**\n",
    "Instead of computing full attention, SSA modifies the standard equation:\n",
    "\n",
    "\\[\n",
    "A_h = \\text{softmax} \\left( \\frac{Q_h K_h^T \\odot S_h}{\\sqrt{d_k}} \\right) V_h\n",
    "\\]\n",
    "\n",
    "where:\n",
    "- \\( S_h \\) is a **sparse mask** that **shifts across heads**.\n",
    "- \\( \\odot \\) represents element-wise multiplication, ensuring **only selected tokens contribute to attention**.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Complexity Analysis**\n",
    "| **Attention Type** | **Computational Complexity** |\n",
    "|--------------------|----------------------------|\n",
    "| **Full Self-Attention** | \\( O(N^2 d) \\) |\n",
    "| **Sparse Attention (Longformer, BigBird)** | \\( O(N \\log N d) \\) |\n",
    "| **Shifted Sparse Attention (SSA)** | \\( O(N d) \\) |\n",
    "\n",
    "Since SSA **reduces the number of attention computations**, it significantly improves efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Pros and Cons of Shifted Sparse Attention**\n",
    "### ‚úÖ **Advantages**\n",
    "- **Reduces memory and computational cost** ‚Üí Works for **long sequences** (e.g., **100K tokens**).\n",
    "- **Retains global dependencies** ‚Üí Unlike naive sparse attention, **shifted heads prevent token loss**.\n",
    "- **Improves efficiency in vision and NLP models**.\n",
    "\n",
    "### ‚ùå **Disadvantages**\n",
    "- **Less expressive than full self-attention**.\n",
    "- **Requires careful tuning** of sparse patterns and shifts.\n",
    "\n",
    "üöÄ **Used in:**\n",
    "- **Swin Transformer (Vision Models)**\n",
    "- **Efficient NLP Models (Long-Context Transformers)**\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Final Summary**\n",
    "| **Feature** | **Standard Self-Attention** | **Sparse Attention** | **Shifted Sparse Attention (SSA)** |\n",
    "|------------|--------------------------|--------------------|--------------------------------|\n",
    "| **Computational Complexity** | \\( O(N^2) \\) | \\( O(N \\log N) \\) | \\( O(N) \\) |\n",
    "| **Captures Global Context?** | ‚úÖ Yes | ‚ö†Ô∏è Partially | ‚úÖ Yes |\n",
    "| **Used For** | NLP, Transformers | Long Sequences | Long Documents, Vision |\n",
    "\n",
    "üöÄ **SSA is a powerful method for handling long sequences efficiently while maintaining long-range dependencies!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
