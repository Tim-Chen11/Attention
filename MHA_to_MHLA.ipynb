{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## **Multi-Head Attention, Multi-Query Attention, and Grouped-Query Attention**\n",
    "These are different **attention mechanisms** designed to balance **performance, speed, and memory efficiency** in transformer models.\n",
    "\n",
    "---\n",
    "\n",
    "# **1. Multi-Head Attention (MHA)**\n",
    "### **What is Multi-Head Attention?**\n",
    "**Multi-Head Attention (MHA)** allows a Transformer to **learn multiple attention patterns simultaneously** by splitting the input into multiple attention heads.\n",
    "\n",
    "### **Mathematical Definition**\n",
    "For an input sequence \\( X \\), we compute:\n",
    "\n",
    "1. Compute multiple **Query (Q), Key (K), and Value (V) matrices**:\n",
    "   \\[\n",
    "   Q_h = X W_Q^h, \\quad K_h = X W_K^h, \\quad V_h = X W_V^h\n",
    "   \\]\n",
    "   for each attention head \\( h \\) (e.g., **8 heads in GPT-3**).\n",
    "\n",
    "2. Compute **attention per head**:\n",
    "   \\[\n",
    "   A_h = \\text{softmax} \\left( \\frac{Q_h K_h^T}{\\sqrt{d_k}} \\right) V_h\n",
    "   \\]\n",
    "\n",
    "3. Concatenate all attention outputs and apply a linear transformation:\n",
    "   \\[\n",
    "   \\text{MultiHead}(X) = W_O \\cdot \\text{Concat}(A_1, A_2, ..., A_H)\n",
    "   \\]\n",
    "\n",
    "### **Pros of Multi-Head Attention**\n",
    "‚úÖ **Enhances model expressiveness** ‚Äì Each head captures different relationships.  \n",
    "‚úÖ **Retains global context** ‚Äì Multiple heads allow attention over various parts of the input.  \n",
    "‚úÖ **Works well for large models** ‚Äì Used in **BERT, GPT, LLaMA, and T5**.\n",
    "\n",
    "### **Cons of Multi-Head Attention**\n",
    "‚ùå **Expensive to compute** ‚Äì Needs **\\( O(HN^2) \\) memory**, where \\( H \\) = number of heads.  \n",
    "‚ùå **Not optimal for real-time inference** ‚Äì Large KV cache storage makes decoding slow.\n",
    "\n",
    "---\n",
    "\n",
    "# **2. Multi-Query Attention (MQA)**\n",
    "### **What is Multi-Query Attention?**\n",
    "**Multi-Query Attention (MQA)** is a modification of MHA where **only one Key (K) and Value (V) pair is shared across all heads**, reducing memory usage.\n",
    "\n",
    "### **Mathematical Definition**\n",
    "Instead of computing separate **\\( K_h, V_h \\)** for each head, we **use a shared Key-Value cache**:\n",
    "\\[\n",
    "Q_h = X W_Q^h, \\quad K_{\\text{shared}} = X W_K, \\quad V_{\\text{shared}} = X W_V\n",
    "\\]\n",
    "\n",
    "Attention computation:\n",
    "\\[\n",
    "A_h = \\text{softmax} \\left( \\frac{Q_h K_{\\text{shared}}^T}{\\sqrt{d_k}} \\right) V_{\\text{shared}}\n",
    "\\]\n",
    "\n",
    "### **Pros of Multi-Query Attention**\n",
    "‚úÖ **Lower memory usage** ‚Äì Only **one Key-Value pair** is stored, saving **KV cache memory**.  \n",
    "‚úÖ **Faster inference** ‚Äì Works well for **large-scale text generation (e.g., ChatGPT, LLaMA-3)**.  \n",
    "‚úÖ **Reduces KV cache size from \\( O(HN) \\) to \\( O(N) \\)**.\n",
    "\n",
    "### **Cons of Multi-Query Attention**\n",
    "‚ùå **Less expressive than MHA** ‚Äì Different heads **attend to the same information**, leading to **less diversity in attention patterns**.  \n",
    "‚ùå **Not optimal for very deep models** ‚Äì **Fine-grained attention is lost** compared to MHA.  \n",
    "\n",
    "üöÄ **Example Models Using MQA**: GPT-4, PaLM (Google‚Äôs LLM)\n",
    "\n",
    "---\n",
    "\n",
    "# **3. Grouped-Query Attention (GQA)**\n",
    "### **What is Grouped-Query Attention?**\n",
    "Grouped-Query Attention (GQA) is a **middle ground** between Multi-Head and Multi-Query Attention.\n",
    "\n",
    "- Instead of **one Key-Value pair** (MQA) or **separate Key-Value pairs per head** (MHA), GQA **groups heads** to share **K and V**.\n",
    "- **Example**: If you have **8 heads**, you can **group them into 4 groups**, meaning **each group shares the same Key-Value pair**.\n",
    "\n",
    "### **Mathematical Definition**\n",
    "For \\( G \\) groups:\n",
    "1. Compute **Query (Q) per head**:\n",
    "   \\[\n",
    "   Q_h = X W_Q^h\n",
    "   \\]\n",
    "2. Compute **shared Key-Value pairs per group**:\n",
    "   \\[\n",
    "   K_g = X W_K^g, \\quad V_g = X W_V^g, \\quad \\text{(where \\( g = 1, ..., G \\))}\n",
    "   \\]\n",
    "\n",
    "3. Compute **attention per head** using the grouped Key-Value:\n",
    "   \\[\n",
    "   A_h = \\text{softmax} \\left( \\frac{Q_h K_g^T}{\\sqrt{d_k}} \\right) V_g\n",
    "   \\]\n",
    "\n",
    "### **Pros of Grouped-Query Attention**\n",
    "‚úÖ **Balances expressiveness and efficiency** ‚Äì Some heads share **K-V**, but not all.  \n",
    "‚úÖ **Lower KV cache memory than MHA** ‚Äì Uses **\\( O(NG) \\) memory instead of \\( O(NH) \\)**.  \n",
    "‚úÖ **More accurate than MQA** ‚Äì Still retains **multiple independent attention heads**.\n",
    "\n",
    "### **Cons of Grouped-Query Attention**\n",
    "‚ùå **Slower than MQA** ‚Äì Still requires multiple KV caches (though less than MHA).  \n",
    "‚ùå **Requires fine-tuning for best results** ‚Äì The **number of groups \\( G \\)** affects performance.  \n",
    "\n",
    "üöÄ **Example Models Using GQA**: **LLaMA-3, Mistral, Gemini**\n",
    "\n",
    "---\n",
    "\n",
    "# **4. Comparison Table**\n",
    "| Feature | Multi-Head Attention (MHA) | Multi-Query Attention (MQA) | Grouped-Query Attention (GQA) |\n",
    "|---------|----------------------------|----------------------------|----------------------------|\n",
    "| **Computation Cost** | High (\\( O(HN^2) \\)) | Low (\\( O(N^2) \\)) | Medium (\\( O(GN^2) \\)) |\n",
    "| **Memory Usage (KV Cache)** | High (\\( O(HN) \\)) | Low (\\( O(N) \\)) | Medium (\\( O(GN) \\)) |\n",
    "| **Inference Speed** | Slowest | Fastest | Faster than MHA, Slower than MQA |\n",
    "| **Expressiveness** | Best (diverse attention patterns) | Worst (all heads share K, V) | Middle ground |\n",
    "| **Best For** | General NLP tasks, BERT | Large-scale LLMs (GPT-4) | Efficient LLaMA-style models |\n",
    "\n",
    "---\n",
    "\n",
    "# **5. Final Summary**\n",
    "- **Multi-Head Attention (MHA)** ‚Üí Best for accuracy but **slow and memory-heavy**.\n",
    "- **Multi-Query Attention (MQA)** ‚Üí **Faster and memory-efficient** but **loses diversity** in attention.\n",
    "- **Grouped-Query Attention (GQA)** ‚Üí **Balances efficiency and accuracy**, used in **modern LLaMA models**.\n",
    "\n",
    "üöÄ **For large-scale models (e.g., GPT-4, Gemini, LLaMA-3), MQA and GQA are preferred due to efficiency improvements.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Multi-Head Latent Attention (MHLA)**\n",
    "### **1. What is Multi-Head Latent Attention?**\n",
    "Multi-Head Latent Attention (MHLA) is an **efficient attention mechanism** that introduces **latent variables** to **reduce the computational complexity** of self-attention. It is primarily used to make **transformers more efficient** for handling **long sequences**.\n",
    "\n",
    "**Key Idea:**  \n",
    "Instead of computing **self-attention over all tokens**, we introduce a **latent representation** \\( L \\) that acts as a **compressed attention space**. This **reduces memory and computation costs** while still capturing important features.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. How Multi-Head Latent Attention Works**\n",
    "MHLA modifies standard **Multi-Head Attention (MHA)** by introducing a **set of latent vectors** that interact with input tokens instead of computing attention directly between all tokens.\n",
    "\n",
    "### **Key Components**\n",
    "1. **Latent Representations \\( L \\)**:  \n",
    "   - These are a fixed number of learnable vectors that **replace direct token-to-token attention**.\n",
    "   - Instead of having **\\( N \\) queries, keys, and values**, we have a **smaller number \\( M \\) of latent vectors**.\n",
    "   - This reduces complexity from **\\( O(N^2) \\)** to **\\( O(NM) \\)**, where \\( M \\ll N \\).\n",
    "\n",
    "2. **Two-Step Attention Mechanism**:  \n",
    "   - **Step 1: Input ‚Üí Latent Attention**  \n",
    "     - The **input tokens attend to latent variables**.  \n",
    "   - **Step 2: Latent ‚Üí Output Attention**  \n",
    "     - The **latent variables attend back to the input tokens**.\n",
    "\n",
    "This structure acts as an **intermediate compression step**, allowing the model to **summarize input information** before re-distributing attention.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Mathematical Formulation of MHLA**\n",
    "### **Step 1: Compute Query, Key, and Value Matrices**\n",
    "For an input sequence **\\( X \\in \\mathbb{R}^{N \\times d} \\)**:\n",
    "- \\( N \\) = Number of input tokens.\n",
    "- \\( d \\) = Embedding dimension.\n",
    "\n",
    "For a **latent representation** **\\( L \\in \\mathbb{R}^{M \\times d} \\)**:\n",
    "- \\( M \\) = Number of latent vectors (**\\( M \\ll N \\)**).\n",
    "\n",
    "Compute **Q, K, V for input tokens**:\n",
    "\\[\n",
    "Q_X = X W_Q, \\quad K_X = X W_K, \\quad V_X = X W_V\n",
    "\\]\n",
    "Compute **Q, K, V for latent vectors**:\n",
    "\\[\n",
    "Q_L = L W_Q^L, \\quad K_L = L W_K^L, \\quad V_L = L W_V^L\n",
    "\\]\n",
    "\n",
    "### **Step 2: Input ‚Üí Latent Attention**\n",
    "Instead of **self-attention between tokens**, we compute attention **from inputs to latent vectors**:\n",
    "\\[\n",
    "A_1 = \\text{softmax} \\left( \\frac{Q_X K_L^T}{\\sqrt{d_k}} \\right) V_L\n",
    "\\]\n",
    "where:\n",
    "- \\( A_1 \\) is the attention from **input tokens to latent space**.\n",
    "- \\( K_L, V_L \\) are **shared latent key-value representations**.\n",
    "\n",
    "### **Step 3: Latent ‚Üí Output Attention**\n",
    "Now, we project **the latent representations back to the token space**:\n",
    "\\[\n",
    "A_2 = \\text{softmax} \\left( \\frac{Q_L K_X^T}{\\sqrt{d_k}} \\right) V_X\n",
    "\\]\n",
    "where:\n",
    "- \\( A_2 \\) is the attention from **latent space back to input tokens**.\n",
    "- \\( Q_L \\) captures **compressed input features**.\n",
    "\n",
    "### **Step 4: Compute Final Output**\n",
    "The final output representation is computed as:\n",
    "\\[\n",
    "\\text{Output} = A_1 + A_2\n",
    "\\]\n",
    "\n",
    "This **compresses global attention into a smaller latent space**, reducing **computational costs**.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Complexity Analysis**\n",
    "| **Method**  | **Computational Complexity** |\n",
    "|------------|----------------------------|\n",
    "| **Standard Self-Attention** | \\( O(N^2 d) \\) |\n",
    "| **Multi-Head Latent Attention (MHLA)** | \\( O(NMd) + O(MNd) = O(NM d) \\) |\n",
    "\n",
    "Since \\( M \\ll N \\), MHLA **significantly reduces memory and compute costs**.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Advantages of Multi-Head Latent Attention**\n",
    "‚úÖ **Faster than traditional self-attention** ‚Üí Replaces quadratic \\( O(N^2) \\) computation with a smaller \\( O(NM) \\).  \n",
    "‚úÖ **Lower memory footprint** ‚Üí Reduces the KV cache size, useful for **long-sequence tasks**.  \n",
    "‚úÖ **Retains long-range dependencies** ‚Üí Latent vectors act as a **compressed summary**.  \n",
    "\n",
    "üöÄ **Used in models like Perceiver and Transformer-XL to handle long inputs efficiently!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
