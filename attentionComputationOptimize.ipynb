{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Sparse Attention & Linear Attention in Transformers**\n",
    "Both **Sparse Attention** and **Linear Attention** are modifications of the standard **self-attention mechanism** in Transformers, designed to improve efficiency when dealing with **long sequences** (e.g., thousands of tokens).\n",
    "\n",
    "---\n",
    "\n",
    "# **1. Standard Self-Attention: Why is it Expensive?**\n",
    "In standard **self-attention**, each token attends to **all other tokens** in the sequence, leading to **quadratic complexity** \\( O(N^2 d) \\), where:\n",
    "- \\( N \\) = sequence length\n",
    "- \\( d \\) = embedding dimension\n",
    "\n",
    "For large sequences (e.g., **GPT-4 handling thousands of tokens**), this is inefficient in terms of **memory and computation**.\n",
    "\n",
    "To **reduce complexity**, researchers developed:\n",
    "1. **Sparse Attention** ‚Üí Reduces the number of tokens attended to.\n",
    "2. **Linear Attention** ‚Üí Reformulates the computation to avoid quadratic complexity.\n",
    "\n",
    "---\n",
    "\n",
    "# **2. Sparse Attention**\n",
    "### **What is Sparse Attention?**\n",
    "Instead of computing attention for **all pairs of tokens**, Sparse Attention selectively allows tokens to **attend only to a subset** of tokens, reducing computational cost.\n",
    "\n",
    "### **Mathematical Explanation**\n",
    "Standard self-attention:\n",
    "\\[\n",
    "A = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "\\]\n",
    "\n",
    "Sparse attention **introduces a sparse matrix \\( S \\) instead of a full softmax mask**:\n",
    "\n",
    "\\[\n",
    "A = \\text{softmax} \\left( \\frac{QK^T \\odot S}{\\sqrt{d_k}} \\right) V\n",
    "\\]\n",
    "\n",
    "where \\( S \\) is a **sparsity mask** that allows **only certain tokens** to interact.\n",
    "\n",
    "---\n",
    "\n",
    "### **Types of Sparse Attention**\n",
    "1. **Fixed-Pattern Sparse Attention**\n",
    "   - Uses **predefined sparsity patterns** (e.g., blocks, local windows).\n",
    "   - Example: **Longformer** allows each token to attend to its **local neighborhood** + some **global tokens**.\n",
    "\n",
    "2. **Learnable Sparse Attention**\n",
    "   - Learns the best **sparsity pattern** dynamically during training.\n",
    "   - Example: **Reformer** uses a **clustering-based** technique (LSH Attention).\n",
    "\n",
    "3. **Strided and Block Sparse Attention**\n",
    "   - Tokens attend at **regular intervals** (strided) or in **blocks**.\n",
    "   - Example: **BigBird** introduces a combination of **global, local, and random attention**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages of Sparse Attention**\n",
    "‚úÖ **Reduces Complexity**: **From \\( O(N^2) \\) to \\( O(N \\log N) \\) or even \\( O(N) \\)**.  \n",
    "‚úÖ **Scales to Long Sequences**: Used in models like **Longformer, BigBird, and Reformer**.  \n",
    "‚úÖ **Efficient Memory Usage**: Only computes attention on **important token interactions**.\n",
    "\n",
    "### **Disadvantages of Sparse Attention**\n",
    "‚ùå **Loss of Global Context**: Since not all tokens attend to each other, some **long-distance dependencies** may be missed.  \n",
    "‚ùå **Hard to Optimize**: Finding the **best sparsity pattern** requires additional design choices.\n",
    "\n",
    "---\n",
    "\n",
    "# **3. Linear Attention**\n",
    "### **What is Linear Attention?**\n",
    "Linear Attention **replaces the softmax operation** in self-attention with a more efficient **kernel-based transformation**, reducing the complexity from **\\( O(N^2) \\) to \\( O(N) \\)**.\n",
    "\n",
    "### **Mathematical Reformulation**\n",
    "Standard self-attention:\n",
    "\\[\n",
    "A = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "\\]\n",
    "\n",
    "Linear attention **rearranges** the operation:\n",
    "\\[\n",
    "A = \\left( \\phi(Q) \\cdot (\\phi(K)^T V) \\right)\n",
    "\\]\n",
    "\n",
    "where **\\( \\phi(x) \\)** is a kernel function (e.g., ReLU or exponential kernel), which enables:\n",
    "\\[\n",
    "\\phi(Q) \\cdot \\left( \\sum \\phi(K) \\cdot V \\right)\n",
    "\\]\n",
    "\n",
    "This allows the computation to be done in **two steps**:\n",
    "1. Compute **context vectors** \\( C = \\sum \\phi(K) V \\) (**linear time**).\n",
    "2. Multiply each query \\( \\phi(Q) \\) by \\( C \\), avoiding \\( O(N^2) \\) complexity.\n",
    "\n",
    "---\n",
    "\n",
    "### **Types of Linear Attention**\n",
    "1. **Kernel-Based Linear Attention**\n",
    "   - Uses special **kernel functions** (e.g., **ReLU, Laplace**) to approximate attention.\n",
    "   - Example: **Performer (Favored Kernel Attention)**.\n",
    "\n",
    "2. **Low-Rank Linear Attention**\n",
    "   - Decomposes attention matrices using **low-rank approximations**.\n",
    "   - Example: **Linformer** reduces the sequence length using a learned projection.\n",
    "\n",
    "3. **Memory-Based Linear Attention**\n",
    "   - Stores **compressed memory representations** instead of computing attention at every step.\n",
    "   - Example: **Synthesizer replaces attention with a learned weight matrix**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages of Linear Attention**\n",
    "‚úÖ **Truly Scales to Large Contexts**: **From \\( O(N^2) \\) to \\( O(N) \\)**.  \n",
    "‚úÖ **Lower Memory Footprint**: Efficient for **long documents (100K+ tokens)**.  \n",
    "‚úÖ **No Need for Sparsity Masks**: Unlike sparse attention, it retains some global context.\n",
    "\n",
    "### **Disadvantages of Linear Attention**\n",
    "‚ùå **Less Expressive**: Approximations may **lose** some relationships in the sequence.  \n",
    "‚ùå **Harder to Train**: Requires careful choice of kernel functions.\n",
    "\n",
    "---\n",
    "\n",
    "# **4. Comparison: Sparse vs. Linear Attention**\n",
    "| Feature            | Sparse Attention | Linear Attention |\n",
    "|--------------------|----------------|----------------|\n",
    "| **Computational Complexity** | \\( O(N \\log N) \\) or \\( O(N) \\) | \\( O(N) \\) |\n",
    "| **Memory Usage**  | Lower than standard | Very low |\n",
    "| **Preserves Global Context?** | Partially | Approximates it |\n",
    "| **Works Well For** | **Documents, Images** | **Very long sequences (100K+ tokens)** |\n",
    "| **Example Models** | **BigBird, Longformer, Reformer** | **Performer, Linformer** |\n",
    "\n",
    "---\n",
    "\n",
    "# **5. When to Use What?**\n",
    "- **Sparse Attention**: **When you want to selectively reduce computation while keeping some global context.**\n",
    "  - Best for **documents (Longformer, BigBird)** or **structured data (Reformer)**.\n",
    "  \n",
    "- **Linear Attention**: **When you need extreme scalability for massive contexts**.\n",
    "  - Best for **long conversations (Performer)** or **scientific literature search (Linformer)**.\n",
    "\n",
    "üöÄ **Both Sparse and Linear Attention are crucial for making transformers efficient for long sequences!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
